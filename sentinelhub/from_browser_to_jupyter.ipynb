{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd90672",
   "metadata": {},
   "source": [
    "# Observing increase in damage done by Wildfires in Greece - from browsing to coding\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The Copernicus Data Space Ecosystem supports learning programming for Earth Observation data analysis in series of incremental steps. This notebook takes you from viewing satellite imagery and experimenting with different visualizations through understanding API requests in a  graphical interface, to writing and editing your own code in Jupyter Notebooks.\n",
    "\n",
    "### Where to start - our use case\n",
    "\n",
    "In our example, we want to analyse the effects of wildfires. Wildfires are becoming more frequent in the wake of climate change, and Earth observation offers far-reaching possibilities for identifying wildfires and quantifying their impact. \n",
    "For this, first we will first look into spectral indices that highlight vegetation fires and burnt areas, then we will find ways to download raster map data of burnt areas, and finally, we will calculate the extent of burnt area the JupyterLab to visualise changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d632f1",
   "metadata": {},
   "source": [
    "## Step 1: Copernicus Browser\n",
    "\n",
    "To get first impression of what wildfires look like on Sentinel-2 imagery, take a look at the extensive forest fires in Canada [here](https://link.dataspace.copernicus.eu/shk) - the link will take you directly to the same scene as the image below, in the Copernicus Browser.\n",
    "\n",
    "![True Color Sentinel-2 image of Wildfires near Lac de la Frégate, Quebec, Canada ](./img/copernicus_browser_canada_wildfire.jpg)\n",
    "\n",
    "You can experiment with different visualizations in the layer list in the sidebar. Which one provides the most intuitive view of the fire activity? Probably the [SWIR visualisation](https://link.dataspace.copernicus.eu/sl6), which is based on bands B12, B8A and B4. It shows the thermal activity of the fire and partly avoids the obscuring effect of the smoke. \n",
    "\n",
    "![SWIR composite Sentinel-2 image of wildfires near Lac de la Frégate, Quebec, Canada ](./img/copernicus_browser_canada_wildfire_swir.jpg)\n",
    "\n",
    "It is also possible to create new custom visualisations using the Browser's composite tool: Here you can drag and drop individual image bands onto the Red, Green and Blue channels or the operands of a spectral index.\n",
    "\n",
    "![Here you can test various visualizations in a graphical interface ](./img/copernicus_browser_canada_wildfire_swir_drag_and_drop.jpg)\n",
    "\n",
    "Finally, you can edit the actual visualisation code in Javascript by using the Custom Script window. Custom scripts - or [evalscripts](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Evalscript.html) as we call them when editing API requests - are short pieces of code that combine a mathematical operation performed on each pixel of the image (essentially a spectral index) and a visualisation that we perform on the results (a palette). Each visualisation layer has its own custom script that can be read and written directly in the Browser. \n",
    "\n",
    "![Here you can edit the visualization code directly](./img/copernicus_browser_canada_wildfire_swir_custom_script.jpg)\n",
    "\n",
    "In addition, there is a large repository of [Custom Scripts](https://custom-scripts.sentinel-hub.com/) that contains evalscript code for many different use cases across a range of satellite datasets and application areas. \n",
    "\n",
    "### Visualizing wildfire effects with the Burnt Area Visualization Custom Script\n",
    "\n",
    "The [Burnt Area Visualization Custom Script](https://custom-scripts.sentinel-hub.com/sentinel-2/burned_area_ms/) combines the Normalized Differential Vegetation Index (NDVI), the Normalized Difference Moisture Index (NDMI) and a custom index using bands B12, B11 and B08 to detect burnt areas.\n",
    "\n",
    "```javascript\n",
    "//VERSION=3\n",
    "// Burned area detection\n",
    "// Author: Monja B. Šebela\n",
    "\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"B12\", \"dataMask\"],\n",
    "        output: { bands: 4 }\n",
    "      };\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "\tvar NDWI=index(samples.B03, samples.B08); \n",
    "\tvar NDVI=index(samples.B08, samples.B04);\n",
    "\tvar INDEX= ((samples.B11 - samples.B12) / (samples.B11 + samples.B12))+(samples.B08);\n",
    "\n",
    "  \tif((INDEX>0.1)||(samples.B02>0.1)||(samples.B11<0.1)||(NDVI>0.3)||(NDWI > 0.1)){\n",
    "  \t\treturn[2.5*samples.B04, 2.5*samples.B03, 2.5*samples.B02, samples.dataMask]\n",
    "\t}\n",
    "\telse {\n",
    " \treturn [1, 0, 0, samples.dataMask]\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "The script creates a small decision tree that combines different thresholds applied to these indices and highlights the areas identified as burnt in red. Copy the script into the Custom Script panel of the Copernicus Browser to see how the burnt areas are highlighted. Try adjusting the individual thresholds to see if you can improve the detection of burnt area!\n",
    "\n",
    "![Visualization of burnt area in Canada using a custom script](./img/copernicus_browser_canada_wildfire_custom_script.jpg)\n",
    "\n",
    "In the next step, you will examine [this](https://link.dataspace.copernicus.eu/951) time series of burnt area from the example of the 2023 wildfires near Alexandropouli, Greece: \n",
    "\n",
    "![Visualization of burnt area in Greece using a custom script](./img/copernicus_browser_greece_wildfire_custom_script.jpg)\n",
    "\n",
    "Note that the custom script in the Browser has an additional output: in the setup section, besides \"default\", \"burnMask\" is also defined, with the values calculated from the decision tree: 0 if the area is not burned, 1 if it is burnt. This property of the script will be useful when we switch to Requests Builder and Jupyter Notebooks.\n",
    "\n",
    "## Step 2: Request Builder\n",
    "\n",
    "Request Builder is an online graphical interface to the Sentinel Hub API-s. This tool makes it easier to create and debug API requests, and supports the export of the resulting code in various programming languages. In this tutorial, we will create a [Process API](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Process.html) request for downloading raster images of the burnt area from the location of the wildfire in Greece that we have already examined in the Browser. Just like a [Process API request in code](https://github.com/eu-cdse/notebook-samples/blob/main/sentinelhub/introduction_to_SH_APIs.ipynb), a request created with the Requests Builder consists of 5 main parts:\n",
    "- `Data Collection`\n",
    "- `Time Range`\n",
    "- `Area of Interest`\n",
    "- `Output`\n",
    "- `Evalscript`\n",
    "\n",
    "![Screenshot of Requests Builder for the Alexandropouli wildfire test location](./img/request_builder_wildfire_script.jpg)\n",
    "\n",
    "These can be set individually in the interface. Use the following settings for the Alexandropouli Wildfire example here:\n",
    "\n",
    "- `Data Collection`: sentinel-2 l2a\n",
    "- `Time Range`: From 12.09.2023 to 13.09.2023\n",
    "- `Area of Interest`: [25.558398, 40.806995, 26.298798, 41.270524]\n",
    "\n",
    "Click `Parse` to parse the area of interest - it should be displayed in the map window and zoom in to the rectangle of interest.\n",
    "\n",
    "- `Evalscript`: use the evalscript from the previous Copernicus Browser example:\n",
    "\n",
    "```javascript\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"B12\", \"dataMask\"],\n",
    "        output: [\n",
    "      \t\t{ id: \"default\", bands: 4 },\n",
    "      \t\t{ id: \"burnMask\", bands: 1, sampleType: \"UINT8\" },\n",
    "    \t]\n",
    "      };\n",
    "}\n",
    "function evaluatePixel(samples) {\n",
    "\tvar NDWI=index(samples.B03, samples.B08); \n",
    "\tvar NDVI=index(samples.B08, samples.B04);\n",
    "\tvar INDEX= ((samples.B11 - samples.B12) / (samples.B11 + samples.B12))+(samples.B08);\n",
    "\n",
    "  \tif((INDEX>0.1)||(samples.B02>0.1)||(samples.B11<0.1)||(NDVI>0.3)||(NDWI > 0.1)){\n",
    "  \t\treturn{\n",
    "            default:[2.5*samples.B04, 2.5*samples.B03, 2.5*samples.B02, samples.dataMask],\n",
    "\t\t\tburnMask:[0]\n",
    "        }\n",
    "\t}\n",
    "\telse {\n",
    " \treturn {\n",
    "        default: [1, 0, 0, samples.dataMask],\n",
    "        burnMask: [1]}\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "- `Output`: Remember, the evalscript has two different outputs, `default` with 4 bands (Red, Green, Blue and Transparency/Data Mask), and `burnMask`, which returns 1 where the area is burnt and 0 where it is not. We need to set up the output so that a raster is created from these two outputs for each image. As the area of interest is rather large and the limit for a single request is 2500×2500 pixels, we will reduce the resolution to 40 meters. First click on the `resolution` tab, then select\n",
    "  - `Res X in meters`: 40\n",
    "  - `Res Y in meters`: 40\n",
    "  - `Image format`: TIFF\n",
    "  - `Identifier`: default\n",
    "\n",
    "Now you need to click on the `Add Response` button to create a second output dataset. This should have the same format and be named according to the second output in the evalscript:\n",
    "  - `Image format`: TIFF\n",
    "  - `Identifier`: burnMask\n",
    "\n",
    "Finally, you must set the language of the Request Preview to `python-requests` in the dropdown menu. This will create request code you can later use in the Jupyter Notebook as well.\n",
    "\n",
    "If you now click on `Send request`, you will be prompted to save the request and download the response. The response is a .tar file containing `default.tif` and `burnMask.tif`.\n",
    "To download raster datasets of the burnt area for a time series, you only need to change the `Time Range` parameter and repeat running the request. Possible dates can be the following:\n",
    "\n",
    "  - 03.08.2023 - 04.08.2023\n",
    "  - 23.08.2023 - 24.08.2023\n",
    "  - 28.08.2023 - 29.08.2023\n",
    "  - 02.09.2023 - 03.08.2023\n",
    "  - 12.08.2023 - 13.08.2023\n",
    "\n",
    "The result is a series of TIFF files, including burnt/non-burnt masks, which you can process locally in GIS software to calculate quantitative results. But more importantly, you can copy the code from the Request Preview window and use it in a Jupyter Notebook of the same test case.\n",
    "\n",
    "## Step 3: Jupyter notebooks\n",
    "\n",
    "### Importing necessary libraries\n",
    "\n",
    "To run this example, you do not need any additional GIS-specific libraries. You can of course improve the workflow with additional libraries, but for someone new to earth observation coding, this basic notebook could be a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauthlib.oauth2 import BackendApplicationClient\n",
    "from requests_oauthlib import OAuth2Session\n",
    "from sentinelhub import SHConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef4d36",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "You can obtain credentials for the Sentinel Hub services (`client_id` & `client_secret`) by navigating to your [Dashboard](https://shapps.dataspace.copernicus.eu/dashboard/#/). In the User Settings, you can create a new OAuth Client to generate these credentials. More detailed instructions can be found on the  corresponding [documentation page](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html).\n",
    "\n",
    "Once you run the next cell, you will be prompted to enter the `client_id` and `client_secret`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba35163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your client credentials\n",
    "\n",
    "config = SHConfig()\n",
    "# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n",
    "# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\n",
    "config.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "config.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n",
    "# config.save(\"cdse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced1e25",
   "metadata": {},
   "source": [
    "If you have the credentials, you will need a session token to make requests. This token is generated in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getauth_token():\n",
    "    # Create a session\n",
    "    client = BackendApplicationClient(client_id=config.sh_client_id)\n",
    "    oauth = OAuth2Session(client=client)\n",
    "    # Get token for the session\n",
    "    token = oauth.fetch_token(\n",
    "        token_url=\"https://identity.cloudferro.com/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "        client_id=config.sh_client_id,\n",
    "        client_secret=config.sh_client_secret,\n",
    "    )\n",
    "    # All requests using this session will have an access token automatically added\n",
    "\n",
    "    return oauth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014255d",
   "metadata": {},
   "source": [
    "## Defining time slots \n",
    "\n",
    "Using the [Requests Builder](https://shapps.dataspace.copernicus.eu/requests-builder/) and the Browser, we can see all the images acquisitions that match our criteria. In the following cell, we enter these time slots to create a time series and understand the extent of the damage caused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7dd6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slots = [\n",
    "    (\"2023-08-03\", \"2023-08-04\"),\n",
    "    (\"2023-08-23\", \"2023-08-24\"),\n",
    "    (\"2023-08-28\", \"2023-08-29\"),\n",
    "    (\"2023-09-02\", \"2023-09-03\"),\n",
    "    (\"2023-09-12\", \"2023-09-13\"),\n",
    "]\n",
    "print(\"Time Slots:\\n\")\n",
    "for slot in slots:\n",
    "    print(slot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4006e96",
   "metadata": {},
   "source": [
    "Next, we can enter the evalscript for Burnt Area Mapping that we used earlier in the Browser and Requests Builder - feel free to copy it from the Browser window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript = \"\"\"\n",
    "//VERSION=3\n",
    "// Burneed area detection\n",
    "// Author: Monja B. Šebela\n",
    "\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"B12\", \"dataMask\"],\n",
    "        output: [\n",
    "      \t\t{ id: \"default\", bands: 3 },\n",
    "      \t\t{ id: \"burnMask\", bands: 1, sampleType: \"UINT8\" },\n",
    "    \t]\n",
    "      };\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "\tvar NDWI=index(samples.B03, samples.B08); \n",
    "\tvar NDVI=index(samples.B08, samples.B04);\n",
    "\tvar INDEX= ((samples.B11 - samples.B12) / (samples.B11 + samples.B12))+(samples.B08);\n",
    "\n",
    "  \tif((INDEX>0.15)||(samples.B02>0.1)||(samples.B11<0.1)||(NDVI>0.3)||(NDWI > 0.1)){\n",
    "  \t\treturn{\n",
    "            default:[2.5*samples.B04, 2.5*samples.B03, 2.5*samples.B02, samples.dataMask],\n",
    "\t\t\tburnMask:[0]\n",
    "        }\n",
    "\t}\n",
    "\telse {\n",
    " \treturn {\n",
    "        default: [1, 0, 0],\n",
    "        burnMask: [1]}\n",
    "\t}\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755c1e8",
   "metadata": {},
   "source": [
    "Here, we have defined a function that contains the request code made in the Requests Builder and references the evalscript defined above. Since we are requesting multiple files in the output, we can expect a compressed .tar file containing the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ebd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(slot):\n",
    "    request = {\n",
    "        \"input\": {\n",
    "            \"bounds\": {\n",
    "                \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n",
    "                \"bbox\": [25.558398, 40.806995, 26.298798, 41.270524],\n",
    "            },\n",
    "            \"data\": [\n",
    "                {\n",
    "                    \"dataFilter\": {\n",
    "                        \"timeRange\": {\n",
    "                            \"from\": slot[0] + \"T00:00:00Z\",\n",
    "                            \"to\": slot[1] + \"T00:00:00Z\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"type\": \"sentinel-2-l2a\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"width\": 1247.7098306086236,\n",
    "            \"height\": 1031.9962449583074,\n",
    "            \"responses\": [\n",
    "                {\n",
    "                    \"identifier\": \"default\",\n",
    "                    \"format\": {\"type\": \"image/tiff\"},\n",
    "                },\n",
    "                {\n",
    "                    \"identifier\": \"burnMask\",\n",
    "                    \"format\": {\"type\": \"image/tiff\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"evalscript\": evalscript,\n",
    "    }\n",
    "\n",
    "    url = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\n",
    "    response = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n",
    "    # print(response.status_code)\n",
    "\n",
    "    if response.status_code in (200,):\n",
    "        with open(f\"tarfile_wildfire_{slot[0]}.tar\", \"wb\") as tarfile:\n",
    "            tarfile.write(response.content)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0e3ec",
   "metadata": {},
   "source": [
    "Now, we are ready to make the request for each of the previously defined time slots. We first get the token created with the OAuth client before making the request and loop through the request for all time slots. This cell might take 10-20 seconds as this is where the requests are actually performed. If it runs successfully, we should see that the compressed .tar files are created in the local folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefdd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "oauth = getauth_token()\n",
    "responses = [get_request(slot) for slot in slots]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b01cb9",
   "metadata": {},
   "source": [
    "To further display and analyse the images further, we need to extract the files. This cell does exactly that and we can see the names of the files in the extracted folder as a result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073aa734",
   "metadata": {},
   "outputs": [],
   "source": [
    "for slot in slots:\n",
    "    # open file\n",
    "    file = tarfile.open(f\"tarfile_wildfire_{slot[0]}.tar\")\n",
    "\n",
    "    # print file names\n",
    "    print(file.getnames())\n",
    "\n",
    "    # extract files\n",
    "    file.extractall(f\"./wildfire_{slot[0]}\")\n",
    "\n",
    "    # close file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5d5ad",
   "metadata": {},
   "source": [
    "Now, we create a series of plots to display the images we have requested. If we look at them together, we can see how the damaged area has increased over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93feaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = 2\n",
    "aspect_ratio = 1000 / 1000\n",
    "subplot_kw = {\"xticks\": [], \"yticks\": [], \"frame_on\": False}\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    ncols=ncols,\n",
    "    nrows=nrows,\n",
    "    figsize=(5 * ncols * aspect_ratio, 5 * nrows),\n",
    "    subplot_kw=subplot_kw,\n",
    ")\n",
    "\n",
    "for idx, slot in enumerate(slots):\n",
    "    img = plt.imread(f\"wildfire_{slot[0]}/default.tif\")\n",
    "    ax = axs[idx // ncols][idx % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{slot[0]}  -  {slot[1]}\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee1807",
   "metadata": {},
   "source": [
    "We make a similar visualization with the binary mask that represents the burnt area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = 2\n",
    "aspect_ratio = 1000 / 1000\n",
    "subplot_kw = {\"xticks\": [], \"yticks\": [], \"frame_on\": False}\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    ncols=ncols,\n",
    "    nrows=nrows,\n",
    "    figsize=(5 * ncols * aspect_ratio, 5 * nrows),\n",
    "    subplot_kw=subplot_kw,\n",
    ")\n",
    "\n",
    "for idx, slot in enumerate(slots):\n",
    "    img = plt.imread(f\"wildfire_{slot[0]}/burnMask.tif\")\n",
    "    ax = axs[idx // ncols][idx % ncols]\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(f\"{slot[0]}  -  {slot[1]}\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a57b8",
   "metadata": {},
   "source": [
    "To calculate the burnt area in square meters, we perform a simple count of the pixels labelled as burnt area. From the binary burnMask, we can define a function that calculates the number of pixels that have been classified as burnt regions and multiplies this by the resolution to get the area. The next cell runs this function that takes the mask and the resolution (in meters) as the input and outputs the total area for each time slot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6440c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def burnt_area(burnMask, resolution):\n",
    "    burn_pixel_count = np.sum(burnMask)\n",
    "    burnt_area = burn_pixel_count * (resolution * resolution) / 1000000\n",
    "\n",
    "    return burnt_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "burnt_area_arr = []\n",
    "for idx, slot in enumerate(slots):\n",
    "    burnMask = plt.imread(f\"wildfire_{slot[0]}/burnMask.tif\")\n",
    "    burnt_area_arr.append(burnt_area(burnMask, resolution=50))\n",
    "    print(\n",
    "        f\"The total burnt area is approximately {round(burnt_area(burnMask,resolution = 50),1)} km\\u00b2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba6a32",
   "metadata": {},
   "source": [
    "We can visualise the increasing damaged area by plotting these burnt pixels on a simple line chart as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcffecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = [slot[0] for slot in slots]\n",
    "x = range(len(slots))\n",
    "plt.plot(range(len(slots)), burnt_area_arr)\n",
    "plt.title(\"Time Series of area burnt in the wildfires.\")\n",
    "plt.xticks(np.arange(0, 5, step=1), xlabels, rotation=30, ha=\"center\")\n",
    "plt.xlabel(\"Time slots\")\n",
    "plt.ylabel(\"Area burnt (in $km^2$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b7a10",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This was a brief guide on how to go from viewing satellite imagery in the Copernicus Browser using [Custom Scripts](https://custom-scripts.sentinel-hub.com/custom-scripts/) and requests from the [Requests Builder](https://shapps.dataspace.copernicus.eu/requests-builder/) to preparing a Jupyter Notebook to begin your analysis. From here, you can analyse the pixels, derive statistics and create a workflow that is suitable for your problem. The next step could be to familiarize yourself with various other Sentinel Hub API-s, using [this notebook example](https://github.com/eu-cdse/notebook-samples/blob/main/sentinelhub/introduction_to_SH_APIs.ipynb). You can find more information about the different APIs and various examples in the [documentation](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub.html). Check out the [Custom Scripts Repository](https://custom-scripts.sentinel-hub.com/custom-scripts/) for more examples of evalscripts for a wide range of applications. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Sentinel Hub",
   "language": "python",
   "name": "sentinelhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
