{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c474f8",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import the necessary libraries for satellite data processing, visualization, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "import getpass\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "\n",
    "from sentinelhub import (\n",
    "    SHConfig,\n",
    "    DataCollection,\n",
    "    SentinelHubRequest,\n",
    "    SentinelHubStatistical,\n",
    "    SentinelHubStatisticalDownloadClient,\n",
    "    BBox,\n",
    "    bbox_to_dimensions,\n",
    "    CRS,\n",
    "    MimeType,\n",
    "    Geometry,\n",
    "    parse_time,\n",
    ")\n",
    "\n",
    "from utils import plot_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab67006",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "\n",
    "Credentials for Sentinel Hub services (`client_id` & `client_secret`) can be obtained in your [Dashboard](https://shapps.dataspace.copernicus.eu/dashboard/#/). In the User Settings you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevant [documentation page](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html).\n",
    "\n",
    "Now that you have your `client_id` & `client_secret`, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found [here](https://sentinelhub-py.readthedocs.io/en/latest/configure.html). Using these instructions you can create a profile specific to using the package for accessing Copernicus Data Space Ecosystem data collections. This is useful as changes to the the config class are usually only temporary in your notebook and by saving the configuration to your profile you won't need to generate new credentials or overwrite/change the default profile each time you rerun or write a new Jupyter Notebook. \n",
    "\n",
    "If you are a first time user of the Sentinel Hub Python package for Copernicus Data Space Ecosystem, you should create a profile specific to the Copernicus Data Space Ecosystem. You can do this in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ef2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you have not created a configuration.\n",
    "\n",
    "config = SHConfig()\n",
    "# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n",
    "# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\n",
    "config.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "config.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n",
    "# config.save(\"cdse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have already configured a profile, uncomment and specify your profile name\n",
    "config = SHConfig(\"cdse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7f6b4",
   "metadata": {},
   "source": [
    "## Statistical API\n",
    "In the Process API examples, we have seen how to obtain satellite imagery. Statistical API can be used in a very similar way. The main difference is that the results of Statistical API are aggregated statistical values of satellite data instead of entire images. In many use cases, such values are all that we need. By using Statistical API we can avoid downloading and processing large amounts of satellite data.\n",
    "\n",
    "All general rules for building evalscripts apply. However, there are some specifics when using evalscripts with the Statistical API:\n",
    "\n",
    "- The `evaluatePixel()` function must, in addition to other output, always return a `dataMask` output. This output defines which pixels are excluded from calculations. For more details and an example, see [here](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Statistical.html).\n",
    "- The default value of sampleType is `FLOAT32`.\n",
    "- The output.bands parameter in the setup() function can be an array. This makes it possible to specify custom names for the output bands and different output `dataMask` for different outputs, see this [example](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Statistical/Examples.html#multiple-outputs-with-different-datamasks-multi-band-output-with-custom-bands-names-and-different-histogram-types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a318b8eb",
   "metadata": {},
   "source": [
    "#### Define Field Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Latvia agricultural field boundaries with crop types\n",
    "fields = \"latvia_fields_wgs84.geojson\"\n",
    "fields_gdf = gpd.read_file(fields)\n",
    "\n",
    "# Convert fields to Web Mercator for proper basemap alignment\n",
    "fields_gdf_wm = fields_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Create a figure to visualize the three crop types\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "# Define colors for each crop type\n",
    "crop_colors = {\n",
    "    \"Wheat winter\": \"#FFD700\",  # Gold\n",
    "    \"Rape winter\": \"#32CD32\",  # Lime green\n",
    "    \"Field beans\": \"#8B4513\",  # Saddle brown\n",
    "}\n",
    "\n",
    "# Plot each crop type with different colors\n",
    "legend_handles = []\n",
    "for crop_type, color in crop_colors.items():\n",
    "    crop_fields = fields_gdf_wm[fields_gdf_wm[\"crop:name_en\"] == crop_type]\n",
    "    if len(crop_fields) > 0:\n",
    "        plot = crop_fields.plot(\n",
    "            ax=ax, color=color, alpha=0.7, edgecolor=\"black\", linewidth=0.5\n",
    "        )\n",
    "        # Create a manual legend entry\n",
    "        import matplotlib.patches as mpatches\n",
    "\n",
    "        patch = mpatches.Patch(\n",
    "            color=color, label=f\"{crop_type} ({len(crop_fields)} fields)\"\n",
    "        )\n",
    "        legend_handles.append(patch)\n",
    "\n",
    "# Set the axis limits to the bounds of the fields (with valid bounds now)\n",
    "bounds = fields_gdf_wm.total_bounds\n",
    "if np.isfinite(bounds).all():\n",
    "    buffer = 1000  # 1km buffer around fields\n",
    "    ax.set_xlim(bounds[0] - buffer, bounds[2] + buffer)\n",
    "    ax.set_ylim(bounds[1] - buffer, bounds[3] + buffer)\n",
    "\n",
    "# Add basemap for context\n",
    "try:\n",
    "    cx.add_basemap(\n",
    "        ax, crs=fields_gdf_wm.crs, source=cx.providers.OpenStreetMap.Mapnik, alpha=0.6\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Could not add basemap: {e}\")\n",
    "\n",
    "# Add legend manually\n",
    "if legend_handles:\n",
    "    ax.legend(handles=legend_handles, loc=\"upper right\", fontsize=12)\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title(\n",
    "    \"Agricultural Fields by Crop Type - Latvia\", fontsize=16, fontweight=\"bold\"\n",
    ")\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91222ada",
   "metadata": {},
   "source": [
    "### Define helper functions to extract statistics from the Statistical API responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to extract statistics for all acquisition dates\n",
    "def extract_stats(date, stat_data):\n",
    "    d = {}\n",
    "    for key, value in stat_data[\"outputs\"].items():\n",
    "        stats = value[\"bands\"][\"B0\"][\"stats\"]\n",
    "        if stats[\"sampleCount\"] == stats[\"noDataCount\"]:\n",
    "            continue\n",
    "        else:\n",
    "            d[\"date\"] = [date]\n",
    "            for stat_name, stat_value in stats.items():\n",
    "                if stat_name == \"sampleCount\" or stat_name == \"noDataCount\":\n",
    "                    continue\n",
    "                else:\n",
    "                    d[f\"{key}_{stat_name}\"] = [stat_value]\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "\n",
    "def read_acquisitions_stats(stat_data):\n",
    "    df_li = []\n",
    "    for aq in stat_data:\n",
    "        date = aq[\"interval\"][\"from\"][:10]\n",
    "        df_li.append(extract_stats(date, aq))\n",
    "    return pd.concat(df_li)\n",
    "\n",
    "\n",
    "def stats_to_df(stats_data):\n",
    "    \"\"\"Transform Statistical API response into a pandas.DataFrame\"\"\"\n",
    "    df_data = []\n",
    "\n",
    "    for single_data in stats_data[\"data\"]:\n",
    "        df_entry = {}\n",
    "        is_valid_entry = True\n",
    "\n",
    "        df_entry[\"interval_from\"] = parse_time(single_data[\"interval\"][\"from\"]).date()\n",
    "        df_entry[\"interval_to\"] = parse_time(single_data[\"interval\"][\"to\"]).date()\n",
    "\n",
    "        for output_name, output_data in single_data[\"outputs\"].items():\n",
    "            for band_name, band_values in output_data[\"bands\"].items():\n",
    "                band_stats = band_values[\"stats\"]\n",
    "                if band_stats[\"sampleCount\"] == band_stats[\"noDataCount\"]:\n",
    "                    is_valid_entry = False\n",
    "                    break\n",
    "\n",
    "                for stat_name, value in band_stats.items():\n",
    "                    col_name = f\"{output_name}_{band_name}_{stat_name}\"\n",
    "                    if stat_name == \"percentiles\":\n",
    "                        for perc, perc_val in value.items():\n",
    "                            perc_col_name = f\"{col_name}_{perc}\"\n",
    "                            df_entry[perc_col_name] = perc_val\n",
    "                    else:\n",
    "                        df_entry[col_name] = value\n",
    "\n",
    "        if is_valid_entry:\n",
    "            df_data.append(df_entry)\n",
    "\n",
    "    return pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed66d2",
   "metadata": {},
   "source": [
    "## 1. How to create an NDVI time series for a field of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbe229",
   "metadata": {},
   "source": [
    "### The Evalscript\n",
    "\n",
    "In this evalscript, we are calculating NDVI. Let's remind ourselves that evalscripts operate slightly differently with Statistical API:\n",
    "\n",
    "- The `evaluatePixel()` function must, in addition to other output, always return a `dataMask` output. This output defines which pixels are excluded from calculations. For more details and an example, see [here](https://docs.sentinel-hub.com/api/latest/api/statistical/#exclude-pixels-from-calculations-datamask-output).\n",
    "- The default value of sampleType is `FLOAT32`.\n",
    "- The output.bands parameter in the setup() function can be an array. This makes it possible to specify custom names for the output bands and different output `dataMask` for different outputs, see this [example](https://docs.sentinel-hub.com/api/latest/api/statistical/examples/#multiple-outputs-with-different-datamasks-multi-band-output-with-custom-bands-names-and-different-histogram-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf932ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript = \"\"\"\n",
    "//VERSION=3\n",
    "function setup() {\n",
    "  return {\n",
    "    input: [{\n",
    "      bands: [\n",
    "        \"B04\",\n",
    "        \"B08\",\n",
    "        \"dataMask\"\n",
    "      ]\n",
    "    }],\n",
    "    output: [\n",
    "      {\n",
    "        id: \"ndvi\",\n",
    "        bands: 1\n",
    "      },\n",
    "      {\n",
    "        id: \"dataMask\",\n",
    "        bands: 1\n",
    "      }]\n",
    "  };\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    let index = (samples.B08 - samples.B04) / (samples.B08+ samples.B04);\n",
    "    return {\n",
    "        ndvi: [index],\n",
    "        dataMask: [samples.dataMask],\n",
    "    };\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec69ac",
   "metadata": {},
   "source": [
    "### The Request Body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9881f8e7",
   "metadata": {},
   "source": [
    "Now we have defined the evalscript and the two fields of interest, we can build the first Statistical API Request, before returning the response for the first field. In this request, as part of the payload we define some input parameters:\n",
    "- `time_interval` this defines the time range of our request.\n",
    "- `aggregation_interval` this defines the length of time each interval is. In this case, the interval is 10 days. The aggregation intervals should be at least one day long (e.g. \"P5D\", \"P30D\"). You can only use period OR time designator not both. \n",
    "\n",
    "**NOTE:**\n",
    "If `time_interval` is not divisible by an aggregationInterval, the last (\"not full\") time interval will be dismissed by default (SKIP option). The user can instead set the lastIntervalBehavior to SHORTEN (shortens the last interval so that it ends at the end of the provided time range) or EXTEND (extends the last interval over the end of the provided time range so that all the intervals are of equal duration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_gdf = gpd.read_file(fields)\n",
    "\n",
    "geometry = Geometry(geometry=fields_gdf.geometry.iloc[0], crs=CRS.WGS84)\n",
    "\n",
    "request = SentinelHubStatistical(\n",
    "    aggregation=SentinelHubStatistical.aggregation(\n",
    "        evalscript=evalscript,\n",
    "        time_interval=(\"2023-01-01T00:00:00Z\", \"2023-12-31T23:59:59Z\"),\n",
    "        aggregation_interval=\"P5D\",\n",
    "        resolution=(0.0001, 0.0001),\n",
    "    ),\n",
    "    input_data=[\n",
    "        SentinelHubStatistical.input_data(\n",
    "            DataCollection.SENTINEL2_L2A.define_from(\n",
    "                \"s2l2a\", service_url=config.sh_base_url\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    geometry=geometry,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response1 = request.get_data()\n",
    "# response1 # try uncommenting this line to see what the raw JSON response looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47e286",
   "metadata": {},
   "source": [
    "### Manipulation and Visualisation of our Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdce9ab",
   "metadata": {},
   "source": [
    "However, as it is clear to see, our response is not that useful in `json` format. It's difficult to read from a human perspective. So, let's transform it into a `pandas` dataframe. To help us achieve this, let's call some helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df1 = read_acquisitions_stats(response1[0][\"data\"])\n",
    "result_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd8ebd",
   "metadata": {},
   "source": [
    "We can take this another step further, and display the data in a time series using the Matplotlib python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stat, ax_stat = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "# Extract data\n",
    "t1 = result_df1[\"date\"]\n",
    "ndvi_mean_field1 = result_df1[\"ndvi_mean\"]\n",
    "ndvi_std_field1 = result_df1[\"ndvi_stDev\"]\n",
    "\n",
    "# Plot mean and standard deviation\n",
    "ax_stat.plot(t1, ndvi_mean_field1, label=\"field 1 mean\")\n",
    "ax_stat.fill_between(\n",
    "    t1,\n",
    "    ndvi_mean_field1 - ndvi_std_field1,\n",
    "    ndvi_mean_field1 + ndvi_std_field1,\n",
    "    alpha=0.3,\n",
    "    label=\"field 1 stDev\",\n",
    ")\n",
    "\n",
    "# Set tick parameters\n",
    "ax_stat.tick_params(axis=\"x\", labelrotation=30, labelsize=12)\n",
    "ax_stat.tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "# Reduce number of x-tick labels\n",
    "ax_stat.xaxis.set_major_locator(mdates.AutoDateLocator(maxticks=20))\n",
    "\n",
    "# Set labels and title\n",
    "ax_stat.set(xlabel=\"Date\", ylabel=\"NDVI/unitless\", title=\"NDVI time series\")\n",
    "\n",
    "# Set legend\n",
    "ax_stat.legend(loc=\"lower right\", prop={\"size\": 12});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6efb1",
   "metadata": {},
   "source": [
    "## 2. Removing cloudy acquisitions from your NDVI time series\n",
    "\n",
    "However, not filtering out the cloudy acquisitions means we have a very noisy time series which is not reflective of the NDVI values on the land surface. In the second example, we will filter out the cloudiest acquisitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e008e5",
   "metadata": {},
   "source": [
    "### The Evalscript\n",
    "\n",
    "This stays the same as the previous example, so we do not need to redefine it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2bf01",
   "metadata": {},
   "source": [
    "### The Request Body\n",
    "\n",
    "The request is only slightly adjusted from previously. We have added an additional argument: \n",
    "\n",
    "```\n",
    "other_args={\"dataFilter\": {\"maxCloudCoverage\": 10}},\n",
    "```\n",
    "This will filter our data and means we will only use Sentinel-2 data where the cloud cover percentage of the scene was below 10%            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_gdf = gpd.read_file(fields)\n",
    "\n",
    "geometry = Geometry(geometry=fields_gdf.geometry.iloc[0], crs=CRS.WGS84)\n",
    "\n",
    "request = SentinelHubStatistical(\n",
    "    aggregation=SentinelHubStatistical.aggregation(\n",
    "        evalscript=evalscript,\n",
    "        time_interval=(\"2023-01-01T00:00:00Z\", \"2023-12-31T23:59:59Z\"),\n",
    "        aggregation_interval=\"P5D\",\n",
    "        resolution=(0.0001, 0.0001),\n",
    "    ),\n",
    "    input_data=[\n",
    "        SentinelHubStatistical.input_data(\n",
    "            DataCollection.SENTINEL2_L2A.define_from(\n",
    "                \"s2l2a\", service_url=config.sh_base_url\n",
    "            ),\n",
    "            other_args={\"dataFilter\": {\"maxCloudCoverage\": 10}},\n",
    "        ),\n",
    "    ],\n",
    "    geometry=geometry,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response1 = request.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db93b9",
   "metadata": {},
   "source": [
    "### Manipulation and Visualisation of our Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df1 = read_acquisitions_stats(response1[0][\"data\"])\n",
    "result_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c24555",
   "metadata": {},
   "source": [
    "We can take this another step further, and display the data in a time series using the Matplotlib python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6f2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stat, ax_stat = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "# Extract data\n",
    "t1 = result_df1[\"date\"]\n",
    "ndvi_mean_field1 = result_df1[\"ndvi_mean\"]\n",
    "ndvi_std_field1 = result_df1[\"ndvi_stDev\"]\n",
    "\n",
    "# Plot mean and standard deviation\n",
    "ax_stat.plot(t1, ndvi_mean_field1, label=\"field 1 mean\")\n",
    "ax_stat.fill_between(\n",
    "    t1,\n",
    "    ndvi_mean_field1 - ndvi_std_field1,\n",
    "    ndvi_mean_field1 + ndvi_std_field1,\n",
    "    alpha=0.3,\n",
    "    label=\"field 1 stDev\",\n",
    ")\n",
    "\n",
    "# Set tick parameters\n",
    "ax_stat.tick_params(axis=\"x\", labelrotation=30, labelsize=12)\n",
    "ax_stat.tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "# Reduce number of x-tick labels\n",
    "ax_stat.xaxis.set_major_locator(mdates.AutoDateLocator(maxticks=20))\n",
    "\n",
    "# Set labels and title\n",
    "ax_stat.set(xlabel=\"Date\", ylabel=\"NDVI/unitless\", title=\"NDVI time series\")\n",
    "\n",
    "# Set legend\n",
    "ax_stat.legend(loc=\"lower right\", prop={\"size\": 12});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb729367",
   "metadata": {},
   "source": [
    "We now have a much smoother NDVI time series. We have less data points now though. In your own time, try experimenting with the cloud coverage filter and see if you can find the sweet point between the number of data points and smoothness of the time series plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aafe73e",
   "metadata": {},
   "source": [
    "We can take this another step further, and display the data in a time series using the Matplotlib python library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dede560",
   "metadata": {},
   "source": [
    "## 3. Outputting multiple outputs from a single statistical API request\n",
    "\n",
    "It is also possible to define multiple outputs from your evalscript. In this example, we generate NDVI and NDWI time series in the same request and then plot them on the same plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4213030",
   "metadata": {},
   "source": [
    "### The Evalscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript = \"\"\"\n",
    "//VERSION=3\n",
    "\n",
    "function setup() {\n",
    "  return {\n",
    "    input: [\n",
    "      {\n",
    "        bands: [\n",
    "          \"B03\",\n",
    "          \"B04\",\n",
    "          \"B08\",\n",
    "          \"dataMask\",\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    output: [\n",
    "      {\n",
    "        id: \"ndvi\",\n",
    "        bands: 1\n",
    "      },\n",
    "      {\n",
    "        id: \"ndwi\",\n",
    "        bands: 1\n",
    "      },  \n",
    "      {\n",
    "        id: \"dataMask\",\n",
    "        bands: 1\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "\n",
    "\n",
    "    return {\n",
    "      ndvi: [index(samples.B08, samples.B04)],\n",
    "      ndwi: [index(samples.B08, samples.B03)],\n",
    "      dataMask: [samples.dataMask]\n",
    "    };\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95c742",
   "metadata": {},
   "source": [
    "### The Request Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d97111",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_gdf = gpd.read_file(fields)\n",
    "\n",
    "field1 = fields_gdf.geometry.values[0]\n",
    "\n",
    "geometry = Geometry(geometry=field1, crs=CRS.WGS84)\n",
    "\n",
    "request = SentinelHubStatistical(\n",
    "    aggregation=SentinelHubStatistical.aggregation(\n",
    "        evalscript=evalscript,\n",
    "        time_interval=(\"2022-05-01T00:00:00Z\", \"2023-04-30T23:59:59Z\"),\n",
    "        aggregation_interval=\"P5D\",\n",
    "        resolution=(0.0001, 0.0001),\n",
    "    ),\n",
    "    input_data=[\n",
    "        SentinelHubStatistical.input_data(\n",
    "            DataCollection.SENTINEL2_L2A.define_from(\n",
    "                \"s2l2a\", service_url=config.sh_base_url\n",
    "            ),\n",
    "            other_args={\"dataFilter\": {\"maxCloudCoverage\": 10}},\n",
    "        ),\n",
    "    ],\n",
    "    geometry=geometry,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response1 = request.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fdeee",
   "metadata": {},
   "source": [
    "### Manipulation and Visualisation of our Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3853616",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df1 = read_acquisitions_stats(response1[0][\"data\"])\n",
    "result_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f334d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stat, ax_stat = plt.subplots(1, 1, figsize=(18, 6))\n",
    "\n",
    "# Extract data\n",
    "t = result_df1[\"date\"]\n",
    "ndvi_mean_field1 = result_df1[\"ndvi_mean\"]\n",
    "ndvi_std_field1 = result_df1[\"ndvi_stDev\"]\n",
    "ndwi_mean_field1 = result_df1[\"ndwi_mean\"]\n",
    "ndwi_std_field1 = result_df1[\"ndwi_stDev\"]\n",
    "\n",
    "# Plot NDVI mean and standard deviation\n",
    "ax_stat.plot(t, ndvi_mean_field1, label=\"NDVI mean\")\n",
    "ax_stat.fill_between(\n",
    "    t,\n",
    "    ndvi_mean_field1 - ndvi_std_field1,\n",
    "    ndvi_mean_field1 + ndvi_std_field1,\n",
    "    alpha=0.3,\n",
    "    label=\"NDVI stDev\",\n",
    ")\n",
    "\n",
    "# Plot NDWI mean and standard deviation\n",
    "ax_stat.plot(t, ndwi_mean_field1, label=\"NDWI mean\")\n",
    "ax_stat.fill_between(\n",
    "    t,\n",
    "    ndwi_mean_field1 - ndwi_std_field1,\n",
    "    ndwi_mean_field1 + ndwi_std_field1,\n",
    "    alpha=0.3,\n",
    "    label=\"NDWI stDev\",\n",
    ")\n",
    "\n",
    "# Set tick parameters\n",
    "ax_stat.tick_params(axis=\"x\", labelrotation=30, labelsize=12)\n",
    "ax_stat.tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "# Reduce number of x-tick labels\n",
    "ax_stat.xaxis.set_major_locator(mdates.AutoDateLocator(maxticks=10))\n",
    "\n",
    "# Set labels and title\n",
    "ax_stat.set(xlabel=\"Date\", ylabel=\"NDVI/unitless\", title=\"NDVI/NDWI time series\")\n",
    "\n",
    "# Set legend\n",
    "ax_stat.legend(loc=\"lower right\", prop={\"size\": 12})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a47bf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored the Sentinel Hub Statistical API for agricultural monitoring using Latvian crop fields. We learned how to:\n",
    "\n",
    "- **Extract time series data** for agricultural fields using NDVI and NDWI indices\n",
    "- **Filter cloudy observations** to improve data quality (10% cloud coverage threshold)\n",
    "- **Calculate multiple vegetation indices** in a single API request for efficiency\n",
    "- **Visualize temporal patterns** with confidence bands showing spatial variability\n",
    "\n",
    "The Statistical API provides an efficient way to monitor crop health and water status without downloading large satellite images. The time series analysis revealed seasonal vegetation patterns typical of agricultural fields, with NDVI showing crop growth cycles and NDWI indicating water content changes throughout the growing season.\n",
    "\n",
    "This approach is particularly valuable for precision agriculture, crop monitoring, and environmental assessment applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sentinel Hub",
   "language": "python",
   "name": "sentinelhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
