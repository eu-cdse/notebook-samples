{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deforestation Monitoring using Sentinel 2 and xarray\n",
    "\n",
    "Sentinel 2 data is one of the most popular satellite dataets, but it does come with challenges. Cloud-free mosaics have to be constructed often in order to get analysis-ready data. Accessing a lot of data through tiles takes a long time, and getting the data into a format it can be easily analysed in with common Python tools can be a challenge.\n",
    "\n",
    "In this notebook, we will show how this whole process of getting analysis-ready data into Python can be sped up by using the Copernicus Dataspace Ecosystem and Sentinel Hub APIs. This is being presented by running through a basic deforestation monitoring use-case. The notebook uses the popular [xarray](https://docs.xarray.dev/en/stable/index.html#) Python library to handle the multidimensional data.\n",
    "\n",
    "What we show in this notebook:\n",
    "\n",
    "- How to access Sentinel 2 data in the Copernicus Dataspace Ecosystem\n",
    "- Calculation of NDVI in the Cloud\n",
    "- Monthly composites\n",
    "- Creating a time series\n",
    "- Loading data into xarray\n",
    "- Basic classification using thresholding\n",
    "- Accuracy assessment of classification\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- [A Copernicus Dataspace Ecosystem account](https://documentation.dataspace.copernicus.eu/Registration.html)\n",
    "- Basic understanding of the Sentinel Hub Processing API ([Introductory Notebook available here](https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/data_download_process_request.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from ipyleaflet import GeoJSON, Map, basemaps\n",
    "from sentinelhub import (\n",
    "    CRS,\n",
    "    BBox,\n",
    "    DataCollection,\n",
    "    MimeType,\n",
    "    SentinelHubDownloadClient,\n",
    "    SentinelHubRequest,\n",
    "    SHConfig,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "To obtain your `client_id` & `client_secret`, you need to navigate to your [Dashboard](https://shapps.dataspace.copernicus.eu/dashboard/#/). In the User Settings, you can create a new OAuth client to generate these credentials. More detailed instructions can be found on the corresponding [documentation page](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html).\n",
    "\n",
    "Now that you have your `client_id` & `client_secret`, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found [here](https://sentinelhub-py.readthedocs.io/en/latest/configure.html). Following these instructions, you can create a profile specifically for using the package to access Copernicus Data Space Ecosystem data collections. This is useful as changes to the config class in your notebook are usually only temporary and by saving the configuration to your profile, you don't have to generate new credentials or overwrite/change the default profile every time you run or write a new Jupyter Notebook. \n",
    "\n",
    "If you are using the Sentinel Hub Python package for the Copernicus Data Space Ecosystem for the first time, you should create a profile specifically for the Copernicus Data Space Ecosystem. You can do this in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you have not created a configuration.\n",
    "\n",
    "config = SHConfig()\n",
    "# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n",
    "# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\n",
    "config.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "config.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n",
    "# config.save(\"cdse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing `profile_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = SHConfig(\"profile_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area of Interest\n",
    "\n",
    "First, we define an area of interest. In this case the area of interest is in the Harz Mountains in Germany since we are aware of substantial forest dieback in recent years.\n",
    "\n",
    "The resolution is defined in the units of the coordinate reference system. Because we want to define units in meters, we also need to define the bounding box coordinates in a CRS using meters. We use EPSG:3035 in this case. This CRS is only available for Europe, outside of Europe we could use EPSG:3857 or UTM Zones.\n",
    "\n",
    "You can also explore the area of interest in the Copernicus Browser [here](https://link.dataspace.copernicus.eu/5t1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired resolution of our data\n",
    "resolution = (100, 100)\n",
    "bbox_coords = [10.633501, 51.611195, 10.787234, 51.698098]\n",
    "epsg = 3035\n",
    "# Convert to 3035 to get crs with meters as units\n",
    "bbox = BBox(bbox_coords, CRS(4326)).transform(epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = bbox.transform(4326).middle\n",
    "\n",
    "# Add OSM background\n",
    "overview_map = Map(basemap=basemaps.OpenStreetMap.Mapnik, center=(y, x), zoom=10)\n",
    "\n",
    "# Add geojson data\n",
    "geo_json = GeoJSON(data=bbox.transform(4326).geojson)\n",
    "overview_map.add_layer(geo_json)\n",
    "\n",
    "# Display\n",
    "overview_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access\n",
    "\n",
    "Next, we define our [evalscript](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Evalscript.html). The evalscript is a piece of JavaScript code that tells the Copernicus Dataspace Ecosystem how to process the pixels you request before they are delivered to you.\n",
    "\n",
    "This makes it a very powerful tool to perform pixel-based calculations in the cloud. For inspiration on what can be done in an evalscript, there is an extensive online resource of community-created evalscripts called [custom-scripts](https://custom-scripts.sentinel-hub.com/). In this example, we want to calculate cloud-free mosaics. This is a perfect application for an evalscript, as you do not have to download the data needed to generate the mosaic, but all calculations are done on the server and only the final cloud-free mosaic is delivered. \n",
    "\n",
    "So let's go over how this is done.\n",
    "\n",
    "The evalscript needs to define two functions, `setup()` and `evaluatePixel()`. First, let's look at the setup function:\n",
    "\n",
    "```js\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"B08\", \"B04\", \"B03\", \"B02\", \"SCL\"],\n",
    "        output: {\n",
    "            bands: 5,\n",
    "            sampleType: \"INT16\"\n",
    "        },\n",
    "        mosaicking: \"ORBIT\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Here we specify which bands we want to request. In this case, we get the bands needed to calculate the NDVI and to display a True Color Image. We also define how our output should be structured, and define the output as a 5-band image with the INT16 data type. \n",
    "\n",
    "Finally, we specify the mosaicking parameter. This determines how the pixel values are returned to us.\n",
    "- `mosaicking: \"SIMPLE\"` returns only a single pixel, either from the most recent, the least recent or the least cloudy Sentinel 2 tile.\n",
    "\n",
    "- `mosaicking: \"ORBIT\"` returns all pixels of unique orbits for the entire time series as a list. We use this to obtain all possible values from which we can create the cloud-free mosaic.\n",
    "\n",
    "Next let's take a look at the `evaluatePixel()` function. This is the function where the actual calculation is defined:\n",
    "\n",
    "```js\n",
    "function evaluatePixel(samples) {\n",
    "    var valid = samples.filter(validate);\n",
    "    if (valid.length > 0 ) {\n",
    "        let cloudless = {\n",
    "            b08: getFirstQuartileValue(valid.map(s => s.B08)),\n",
    "            b04: getFirstQuartileValue(valid.map(s => s.B04)),\n",
    "            b03: getFirstQuartileValue(valid.map(s => s.B03)),\n",
    "            b02: getFirstQuartileValue(valid.map(s => s.B02)),\n",
    "        }\n",
    "        let ndvi = ((cloudless.b08 - cloudless.b04) / (cloudless.b08 + cloudless.b04))\n",
    "        // This applies a scale factor so the data can be saved as an int\n",
    "        let scale = [cloudless.b04, cloudless.b03, cloudless.b02, ndvi].map(v => v*10000);\n",
    "        return scale\n",
    "    }\n",
    "    // If there isn't enough data, return NODATA\n",
    "    return [-32768, -32768, -32768, -32768]\n",
    "}\n",
    "```\n",
    "\n",
    "The way we construct the cloud free mosaic is by first filtering all the available acquisitions to only include the ones which contain clear data with `samples.filter(validate);`. Then we sort the array and get the value at the first quartile of the array. Getting the first quartile instead of the mean or median further reduces the risk that we select a cloudy pixel.\n",
    "\n",
    "Finally, we calculate the NDVI using the cloud-free values and return all the desired values as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript_cloudless = \"\"\"\n",
    "//VERSION=3\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"B08\", \"B04\", \"B03\", \"B02\", \"SCL\"],\n",
    "        output: {\n",
    "            bands: 4,\n",
    "            sampleType: \"INT16\"\n",
    "        },\n",
    "        mosaicking: \"ORBIT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "function getFirstQuartileValue(values) {\n",
    "    values.sort((a,b) => a-b);\n",
    "    return getFirstQuartile(values);\n",
    "}\n",
    "\n",
    "function getFirstQuartile(sortedValues) {\n",
    "    var index = Math.floor(sortedValues.length / 4);\n",
    "    return sortedValues[index];\n",
    "}\n",
    "\n",
    "function validate(sample) {\n",
    "    // Define codes as invalid:\n",
    "    const invalid = [\n",
    "        0, // NO_DATA\n",
    "        1, // SATURATED_DEFECTIVE\n",
    "        3, // CLOUD_SHADOW\n",
    "        7, // CLOUD_LOW_PROBA\n",
    "        8, // CLOUD_MEDIUM_PROBA\n",
    "        9, // CLOUD_HIGH_PROBA\n",
    "        10 // THIN_CIRRUS\n",
    "    ]\n",
    "    return !invalid.includes(sample.SCL)\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    var valid = samples.filter(validate);\n",
    "    if (valid.length > 0 ) {\n",
    "        let cloudless = {\n",
    "            b08: getFirstQuartileValue(valid.map(s => s.B08)),\n",
    "            b04: getFirstQuartileValue(valid.map(s => s.B04)),\n",
    "            b03: getFirstQuartileValue(valid.map(s => s.B03)),\n",
    "            b02: getFirstQuartileValue(valid.map(s => s.B02)),\n",
    "        }\n",
    "        let ndvi = ((cloudless.b08 - cloudless.b04) / (cloudless.b08 + cloudless.b04))\n",
    "        // This applies a scale factor so the data can be saved as an int\n",
    "        let scale = [cloudless.b04, cloudless.b03, cloudless.b02, ndvi].map(v => v*10000);\n",
    "        return scale\n",
    "    }\n",
    "    // If there isn't enough data, return NODATA\n",
    "    return [-32768, -32768, -32768, -32768]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined how the pixels should be handled. However, we still need to define some other parameters to get a full request. \n",
    "\n",
    "We need to define which data we want to use and the timeframe of the data.\n",
    "\n",
    "This is what we are doing in the next cell. Here, we also start building our time series. To see changes over the years, we want to get cloud-free mosaics for the same 3 months over the years. We do this by defining the three months (June-August) in the `interval_of_interest()` function. Then we define a function `get_request()`, which will build the request to the Sentinel Hub API on the Copernicus Data Space Ecosystem.\n",
    "\n",
    "In this [`SentinelHubRequest`](https://sentinelhub-py.readthedocs.io/en/latest/reference/sentinelhub.api.process.html#sentinelhub.api.process.SentinelHubRequest), we define the input data, the timeframe, the output type (TIFF), the bounding box, the resolution and where to save the data.\n",
    "\n",
    "We define this as a function because we want to make several requests with the changing years being the only input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_of_interest(year):\n",
    "    return (datetime(year, 6, 1), datetime(year, 9, 1))\n",
    "\n",
    "\n",
    "def get_request(year):\n",
    "    time_interval = interval_of_interest(year)\n",
    "    return SentinelHubRequest(\n",
    "        evalscript=evalscript_cloudless,\n",
    "        input_data=[\n",
    "            SentinelHubRequest.input_data(\n",
    "                data_collection=DataCollection.SENTINEL2_L2A.define_from(\n",
    "                    \"s2\", service_url=config.sh_base_url\n",
    "                ),\n",
    "                time_interval=time_interval,\n",
    "            )\n",
    "        ],\n",
    "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "        bbox=bbox,\n",
    "        resolution=resolution,\n",
    "        config=config,\n",
    "        data_folder=\"./data\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell now creates a request for each of the years, from 2018 to 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of requests\n",
    "sh_requests = {}\n",
    "for year in range(2018, 2024):\n",
    "    sh_requests[year] = get_request(year)\n",
    "\n",
    "sh_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to download the data. This is done with the utility function `SentinelHubDownloadClient`. It downloads a list of requests in parallel, greatly improving the download speed. Before we can do that, we need to change the format of the requests slightly, which is done in the variable `list_of_requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_requests = [request.download_list[0] for request in sh_requests.values()]\n",
    "\n",
    "# download data with multiple threads\n",
    "data = SentinelHubDownloadClient(config=config).download(\n",
    "    list_of_requests, max_threads=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the requests do not provide any information about which year the data is from, so we rename the output of each request to the year of the data it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_output_path(request):\n",
    "    # Gets the full path to the output from a request\n",
    "    return Path(request.data_folder, request.get_filename_list()[0])\n",
    "\n",
    "\n",
    "# Moves and renames the files to the root directory of results\n",
    "for year, request in sh_requests.items():\n",
    "    request_output_path(request).rename(f\"./data/{year}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data with xarray\n",
    "\n",
    "Now we can load the data into [xarray](https://docs.xarray.dev/en/stable/). We use [rioxarray](https://corteva.github.io/rioxarray/html/index.html), an extension for xarray, to load multiple tiffs into a single xarray dataset.\n",
    "xarray is a scalable tool for analysing multidimensional data in Python. This makes xarray ideal for analysing time series data.\n",
    "\n",
    "The different files correspond to the time dimension, but xarray does not know which file is which time step. Therefore, we add a pre-processing step in which we parse out the year from the filename and add it as the time dimension for that file. \n",
    "\n",
    "The warnings in the output can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_dim(xda):\n",
    "    # This pre-processes the file to add the correct\n",
    "    # year from the filename as the time dimension\n",
    "    year = int(Path(xda.encoding[\"source\"]).stem)\n",
    "    return xda.expand_dims(year=[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = Path(\"./data\").glob(\"*.tif\")\n",
    "ds_s2 = xr.open_mfdataset(\n",
    "    tiff_paths,\n",
    "    engine=\"rasterio\",\n",
    "    preprocess=add_time_dim,\n",
    "    band_as_variable=True,\n",
    ")\n",
    "ds_s2 = ds_s2.rename(\n",
    "    {\n",
    "        \"band_1\": \"R\",\n",
    "        \"band_2\": \"G\",\n",
    "        \"band_3\": \"B\",\n",
    "        \"band_4\": \"NDVI\",\n",
    "    }\n",
    ")\n",
    "ds_s2 = ds_s2 / 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in an xarray dataset with 3 coordinates: year, x and y, as well as the data variables returned by the evalscript as data variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use xarray to plot the RGB data as a true color image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RGB data for a year\n",
    "plot_year = 2018\n",
    "true_color = ds_s2.sel(year=plot_year)[[\"R\", \"G\", \"B\"]].to_array()\n",
    "# Divide by scale factor and apply gamma to brighten image\n",
    "(true_color * 4).plot.imshow()\n",
    "plt.title(f\"True Color {plot_year}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also similarly plot the NDVI values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2.NDVI.plot(cmap=\"PRGn\", x=\"x\", y=\"y\", col=\"year\", col_wrap=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "For analysis the first step is to classify pixels as forest. In our case we will just do a simple thresholding classification where we classify everything above a certain threshold as forest. This isn't the best approach for classifying forest, since agricultural areas can also easily reach very high NDVI values. A better approach would be to classify based on the temporal signature of the pixel. \n",
    "\n",
    "However, for this basic analysis, we stick to the simple thresholding approach.\n",
    "\n",
    "In this case we classify everything above an NDVI of 0.7 as forest. This calculated forest mask is then saved to a new Data Variable in the xarray dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2[\"FOREST\"] = ds_s2.NDVI > 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this forest mask we can already do a quick preliminary analysis to plot the total forest area over the years.\n",
    "\n",
    "To do this we sum up the pixels along the x and y coordinate but not along the time coordinate. This will leave us with one value per year representing the number of pixels classified as forest. We can then calculate the forest area by multiplying the number of forest pixels by the resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_km2(dataarray, resolution):\n",
    "    # Calculate forest area\n",
    "    return dataarray * np.prod(list(resolution)) / 1e6\n",
    "\n",
    "\n",
    "forest_pixels = ds_s2.FOREST.sum([\"x\", \"y\"])\n",
    "forest_area_km2 = to_km2(forest_pixels, resolution)\n",
    "forest_area_km2.plot()\n",
    "plt.title(\"Forest Cover\")\n",
    "plt.ylabel(\"Forest Cover [km²]\")\n",
    "plt.ylim(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the total forest area in this AOI decreased from around 80 km² in 2018 to only around 50 km² in 2023.\n",
    "\n",
    "The next step is to make change maps from year to year. To do this we basically take the difference of the forest mask of a year with its previous year.\n",
    "\n",
    "This will result in 0 value where there has been no change, -1 where forest was lost and +1 where forest was gained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make change maps of forest loss and forest gain compared to previous year\n",
    "\n",
    "# 0 - 0 = No Change: 0\n",
    "# 1 - 1 = No Change: 0\n",
    "# 1 - 0 = Forest Gain: 1\n",
    "# 0 - 1 = Forest Loss: -1\n",
    "\n",
    "# Define custom colors and labels\n",
    "colors = [\"darkred\", \"white\", \"darkblue\"]\n",
    "labels = [\"Forest Loss\", \"No Change\", \"Forest Gain\"]\n",
    "\n",
    "# Create a colormap and normalize it\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "norm = plt.Normalize(-1, 1)  # Adjust the range based on your data\n",
    "\n",
    "plot_year = 2022\n",
    "ds_s2[\"CHANGE\"] = ds_s2.FOREST.astype(int).diff(\"year\", label=\"upper\")\n",
    "ds_s2.CHANGE.sel(year=plot_year).plot(cmap=cmap, norm=norm, add_colorbar=False)\n",
    "\n",
    "# Create a legend with string labels\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=color, label=label) for color, label in zip(colors, labels)\n",
    "]\n",
    "plt.legend(handles=legend_patches, loc=\"lower left\")\n",
    "plt.title(f\"Forest Change Map {plot_year}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the spatial distribution of areas affected by forest loss. In the displayed change from 2021 to 2022, most of the forest loss happened in the northern part of the study area, while the southern part lost comparatively less forest.\n",
    "\n",
    "To get a feel for the loss per year, we can cumulatively sum up the lost areas over the years. This should basically follow the same trends as the earlier plot of total forest area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest Loss per Year\n",
    "forest_loss = (ds_s2.CHANGE == -1).sum([\"x\", \"y\"])\n",
    "forest_loss_km2 = to_km2(forest_loss, resolution)\n",
    "forest_loss_km2.cumsum().plot()\n",
    "plt.title(\"Cumulative Forest Loss\")\n",
    "plt.ylabel(\"Forest Loss [km²]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there have been two years with particularly large amounts of lost forest area. From 2019-2020 and with by far the most lost area between 2021 and 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Finally, we want to see how accurate our data is compared to the widely used Hansen Global Forest Change data. In a real scientific scenario, we would use Ground Truth data to assess the accuracy of our classification. In this case we use the Global Forest Change data in place of Ground Truth data, just to show how an accuracy assessment can be done. The assessment we are doing only shows how accurately we replicate the Global Forest Change data, however we will not know if our product is more or less accurate. For a more accurate assessment, actual Ground Truth data is required.\n",
    "\n",
    "First we download the Global Forest Change Data [here](https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/download.html) and open it using xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./data/\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "hansen_filename = \"Hansen_GFC-2022-v1.10_lossyear_60N_010E.tif\"\n",
    "comp_data = data_path / hansen_filename\n",
    "\n",
    "with comp_data.open(\"wb\") as fs:\n",
    "    hansen_data = requests.get(\n",
    "        f\"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/{hansen_filename}\"\n",
    "    )\n",
    "    fs.write(hansen_data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file\n",
    "ground_truth = (\n",
    "    xr.open_dataarray(comp_data, engine=\"rasterio\")\n",
    "    .rio.clip_box(*bbox_coords)\n",
    "    .rio.reproject(epsg)\n",
    "    .sel(band=1)\n",
    "    .where(lambda gt: gt < 100, 0)  # fill no-data (values over 100) with 0\n",
    ")\n",
    "ground_truth.plot(levels=range(25), cbar_kwargs={\"label\": \"Year of Forest Loss\"})\n",
    "plt.title(\"Global Forest Watch Data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows in which year forest was first lost. To compare with our own data, we need to add the data to our dataset. To do this the data needs to have the same coordinates. This can be achieved with `.interp_like()`. This function interpolates the data to match up the coordinates of another dataset.\n",
    "\n",
    "In this case we chose the interpolation method `nearest` since it is categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2[\"GROUND_TRUTH\"] = ground_truth.interp_like(ds_s2, method=\"nearest\").astype(int)\n",
    "ds_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth data saves the year when deforestation was first detected for a pixel in a single raster. To do this, it encodes the year of forest loss as an integer, giving the year. So, an integer 21 means the pixel was first detected as deforested in 2021, whereas a value of 0 means that deforestation was never detected.\n",
    "\n",
    "Currently our classification saves the deforestation detection in multiple rasters, one for each year. To get our data into a format that is similar to our comparison data we need to convert our rasters for each time step into a single one.\n",
    "\n",
    "To do this we first assign all pixels which were detected as deforestation (`CHANGE == -1`) to the year in which the deforestation was detected (`lost_year`). Then we compute over our time-series the first occurence of deforestation (equivalent to the first non-zero value) per pixel. This is then saved in a new data variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lost forest (-1) into the year it was lost\n",
    "lost_year = (ds_s2.CHANGE == -1) * ds_s2.year % 100\n",
    "first_nonzero = (lost_year != 0).argmax(axis=0).compute()\n",
    "ds_s2[\"LOST_YEAR\"] = lost_year[first_nonzero]\n",
    "ds_s2.LOST_YEAR.plot(levels=range(25), cbar_kwargs={\"label\": \"Year of Forest Loss\"})\n",
    "plt.title(\"Classification Forest Loss Year\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this visually to the Global Forest Watch data, allows us to do some initial quality assessment. We can see definite differences between the two datasets. The Global Forest Watch data has much more clearly defined borders. In general, our classification seems to overestimate deforestation. However, the general pattern of forest loss is the same in both. Most of the deforestation is in the north of the study area, with less forest loss in the south.\n",
    "\n",
    "There are a few reasons for those differences. The main difference has to be in our much more simple approach to forest classification and change detection. It is expected that our approach will lead to large amounts of commission errors since changes are only confirmed using a single observation. It however can also lead to a lot of omission errors since the NDVI thresholding might classify highly productive non-forest areas as forest due to their high NDVI values. \n",
    "\n",
    "However, there are also some systematic differences. Our algorithm looks at differences between the middle of the years, which means that some changes can happen at the end of the growing year which will be detected first in the next year whereas the Global Forest Watch dataset will detect it in the correct (earlier) year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2.GROUND_TRUTH.plot(levels=range(25), cbar_kwargs={\"label\": \"Year of Forest Loss\"})\n",
    "plt.title(\"Global Forest Watch - Interpolated\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also calculate an accuracy score. This is a score from 0-1, where values close to 0.5 basically mean that the classification is random, and values close to 1 mean that most of the values of our comparison data and classification data match.\n",
    "\n",
    "First, we look at the overall accuracy of forest loss over the entire period from 2018 to 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(\n",
    "    (ds_s2.LOST_YEAR > 18).values.ravel(), (ds_s2.GROUND_TRUTH > 18).values.ravel()\n",
    ")\n",
    "print(f\"The overall accuracy of forest loss detection is {score:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from the visual interpretation, with an accuracy of 0.77, our product differs quite a lot compared to the Global Forest Watch data. From this we do not know for sure that our product is less accurate compared to the actual forest loss patterns observed on the ground. We only know that it is different to the Global Forest Watch product. It might be more or less accurate. \n",
    "\n",
    "However, because of the simplicity of our algorithm, it is safe to assume that our output is less accurate. \n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook showed how to efficiently access data stored on the Copernicus Dataspace Ecosystem using the Sentinel Hub APIs. This includes generating cloud-free mosaics and calculating spectral indices in the cloud. \n",
    "\n",
    "It also showed how to import this data using xarray and carry out a basic multi-temporal detection of forest loss.\n",
    "\n",
    "This notebook should serve as a starting point for your own analysis using the powerful Python Data Analysis ecosystem and leveraging the Copernicus Data Space Ecosystem APIs for quick satellite data access."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sentinel Hub",
   "language": "python",
   "name": "sentinelhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
