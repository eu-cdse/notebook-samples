{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3ade02",
   "metadata": {},
   "source": [
    "This notebook presents the use of embeddings from the MajorTom dataset. They were extracted from Sentinel-2 L1C satellite imagery. These embeddings can be used for tasks like classification, regression and change detection. The example demonstrates how embeddings facilitate  efficient processing of large satellite datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b2a060",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bed408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.fs as fs\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import branca.colormap as bcm\n",
    "from staticmap import StaticMap, CircleMarker\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa523e4a",
   "metadata": {},
   "source": [
    "## Defining a bounding box and searching for them in a parquet file In this example, the data covers the area around Girona. The task is to classify the land into three categories: forests, highly urbanized areas, low urbanization/farmland\n",
    "\n",
    "## Steps:\n",
    "## 1. Connecting to the S3 service\n",
    "## 2. Defining AOI\n",
    "## 3. Filtering data based on the AOI"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='https://eodata.dataspace.copernicus.eu',\n",
    "#    aws_access_key_id='YOUR KEY',\n",
    "#    aws_secret_access_key='YOUR ACCESS',\n",
    "    region_name='default'\n",
    ")\n",
    "\n",
    "bucket_name = \"eodata\"\n",
    "prefix = \"auxdata/MajorTOM/embeddings/Core-S2L1C-SSL4EO/\"\n",
    "\n",
    "# your bounding box (AOI) - Girona (example)\n",
    "lon_min, lon_max =  2.78, 2.9\n",
    "lat_min, lat_max = 41.9, 42\n",
    "\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "parquet_files = [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith(\".parquet\")]\n",
    "\n",
    "s3_fs = fs.S3FileSystem(\n",
    "#    access_key='YOUR KEY',\n",
    "#    secret_key='YOUR ACCESS',\n",
    "    endpoint_override='https://eodata.dataspace.copernicus.eu'\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for key in parquet_files:\n",
    "    s3_path = f\"{bucket_name}/{key}\"\n",
    "    dataset = ds.dataset(s3_path, filesystem=s3_fs, format=\"parquet\")\n",
    "\n",
    "    table = dataset.to_table(\n",
    "        filter=(\n",
    "            (ds.field(\"centre_lon\") >= lon_min) &\n",
    "            (ds.field(\"centre_lon\") <= lon_max) &\n",
    "            (ds.field(\"centre_lat\") >= lat_min) &\n",
    "            (ds.field(\"centre_lat\") <= lat_max)\n",
    "        ),\n",
    "        columns=[\"grid_cell\", \"centre_lon\", \"centre_lat\"]\n",
    "    )\n",
    "\n",
    "    if table.num_rows > 0:\n",
    "        df_tmp = table.to_pandas()\n",
    "        df_tmp[\"parquet_file\"] = key\n",
    "        results.append(df_tmp)\n",
    "\n",
    "\n",
    "if results:\n",
    "    df_result = pd.concat(results, ignore_index=True)\n",
    "    df_result = df_result.drop_duplicates()\n",
    "    print(\"Found data\")\n",
    "\n",
    "else:\n",
    "    print(\"No grid cells were found in the bounding box\")"
   ],
   "id": "2146f7f7ff7bd5bf"
  },
  {
   "cell_type": "markdown",
   "id": "52142be2",
   "metadata": {},
   "source": [
    "## Get list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    df_result = pd.concat(results, ignore_index=True)\n",
    "    df_result = df_result.drop_duplicates()\n",
    "\n",
    "    unique_parquets = df_result['parquet_file'].unique()\n",
    "    print(\"List of files:\")\n",
    "    for f in unique_parquets:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"No grid cells were found in the bounding box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7db01",
   "metadata": {},
   "source": [
    "## Loading filtred \n",
    " \n",
    "Steps:\n",
    "1. Connecting to the S3 service\n",
    "2. Loading filtred parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    's3',\n",
    "    endpoint_url='https://eodata.dataspace.copernicus.eu',\n",
    "#    aws_access_key_id='YOUR KEY',\n",
    "#    aws_secret_access_key='YOUR ACCESS',\n",
    "    region_name='default'\n",
    ")  \n",
    "all_dfs = []\n",
    "\n",
    "for parquet_file in df_result['parquet_file'].unique():\n",
    "\n",
    "    \n",
    "    obj = s3.Object(bucket_name, parquet_file)\n",
    "    file_stream = io.BytesIO(obj.get()['Body'].read())\n",
    "    \n",
    "    table = pq.read_table(file_stream)\n",
    "    df_parquet = table.to_pandas()\n",
    "    \n",
    "    grid_cells = df_result[df_result['parquet_file'] == parquet_file]['grid_cell'].unique()\n",
    "    df_filtered = df_parquet[df_parquet['grid_cell'].isin(grid_cells)].copy()\n",
    "    \n",
    "    all_dfs.append(df_filtered)\n",
    "\n",
    "\n",
    "df_embeddings = pd.concat(all_dfs, ignore_index=True)\n",
    "for grid, group in df_embeddings.groupby('grid_cell'):\n",
    "    group = group.sort_values(['centre_lon', 'centre_lat'], ascending=[True, False])\n",
    "    embeds = np.stack(group['embedding'].values)\n",
    "    idxs = group.index\n",
    "\n",
    "    if embeds.shape[0] != 25:\n",
    "        continue\n",
    "\n",
    "    mat = embeds.reshape(5, 5, -1)\n",
    "    mat_T = mat.transpose(1, 0, 2)\n",
    "\n",
    "    df_embeddings.loc[idxs, 'embedding'] = pd.Series(\n",
    "        list(mat_T.reshape(25, -1)), index=idxs\n",
    "    )\n",
    "print(f\"Loaded filtered parquets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebbc5b",
   "metadata": {},
   "source": [
    "## Analytical module\n",
    "\n",
    "In this example there are presented the possibilities of classifying areas by similarity\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Collecting all embeddings into the matrix \n",
    "\n",
    "2. Computing cosine similarity between embeddings\n",
    "\n",
    "3. Using SpectralClustering method to split the points into n-groups based on how similar they are\n",
    "\n",
    "4. Saving the results in the new column\n",
    "\n",
    "5. Reducing the embeddings to two dimension with PCA\n",
    "\n",
    "6. Visualisation of the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd929285",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(df_embeddings['embedding'].values)\n",
    "cos_mat = cosine_similarity(X)\n",
    "cos_mat[cos_mat < 0] = 0\n",
    "\n",
    "\n",
    "n_clusters =3\n",
    "spectral = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "labels = spectral.fit_predict(cos_mat)\n",
    "df_embeddings['cluster'] = labels\n",
    "\n",
    "cmap = cm.get_cmap('tab20', n_clusters)\n",
    "label_to_color = {lbl: colors.rgb2hex(cmap(i)) for i, lbl in enumerate(range(n_clusters))}\n",
    "df_embeddings['color'] = df_embeddings['cluster'].map(label_to_color)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=df_embeddings['color'], alpha=0.7, s=20)\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.title('PCA - Spectral Clustering')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "def plot_clusters_staticmap(df):\n",
    "    m = StaticMap(1800, 800, url_template='http://a.tile.openstreetmap.org/{z}/{x}/{y}.png')\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        m.add_marker(CircleMarker((row.centre_lon, row.centre_lat), row.color, 12))\n",
    "\n",
    "    image = m.render(zoom=10)\n",
    "    image.show() \n",
    "\n",
    "plot_clusters_staticmap(df_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_spectral = folium.Map(\n",
    "    location=[df_embeddings['centre_lat'].mean(), df_embeddings['centre_lon'].mean()],\n",
    "    zoom_start=10,\n",
    "    tiles='CartoDB positron'\n",
    ")\n",
    "\n",
    "for idx, row in df_embeddings.iterrows():\n",
    "    folium.Circle(\n",
    "        location=[row['centre_lat'], row['centre_lon']],\n",
    "        radius=500,\n",
    "        color=row['color'],\n",
    "        fill=True,\n",
    "        fill_color=row['color'],\n",
    "        fill_opacity=0.7,\n",
    "        weight=0,\n",
    "        tooltip=f\"Cluster: {row['cluster']}\"\n",
    "    ).add_to(m_spectral)\n",
    "\n",
    "display(m_spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9d6c4",
   "metadata": {},
   "source": [
    "## Presentation of random images from different clusters. The images come from MajorTOM resources.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Finding the correct S3 folder for each parquet file and load available thumbnails\n",
    "\n",
    "2. Picking a few random examples from each cluster and crop the images to the specified area\n",
    "\n",
    "3. Showing the images with their coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abddbf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='https://eodata.dataspace.copernicus.eu',\n",
    "#    aws_access_key_id='YOUR KEY',\n",
    "#    aws_secret_access_key='YOUR ACCESS',\n",
    "    region_name='default'\n",
    ")\n",
    "bucket_name = \"eodata\"\n",
    "\n",
    "def resolve_core_folder(parquet_path: str) -> str:\n",
    "    parquet_name = os.path.basename(parquet_path).replace(\".parquet\", \"\")\n",
    "    m = re.match(r\"part_(\\d+)-(\\d+)\", parquet_name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Error - parquet name: {parquet_name}\")\n",
    "    start = int(m.group(1))\n",
    "\n",
    "    core_prefix = \"auxdata/MajorTOM/Core-S2L1C/\"\n",
    "    resp = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=core_prefix, Delimiter=\"/\")\n",
    "    for cp in resp.get(\"CommonPrefixes\", []):\n",
    "        folder = cp[\"Prefix\"].rstrip(\"/\")\n",
    "        base = os.path.basename(folder)\n",
    "        m2 = re.match(r\"part_(\\d+)-(\\d+)\", base)\n",
    "        if not m2:\n",
    "            continue\n",
    "        left, right = int(m2.group(1)), int(m2.group(2))\n",
    "        if left <= start <= right:\n",
    "            return folder + \"/\"\n",
    "    raise ValueError(f\"Not found: {parquet_path}\")\n",
    "\n",
    "def load_thumbnail_from_s3(grid_cell, core_folders):\n",
    " \n",
    "    for folder in core_folders:\n",
    "        key = f\"{folder}{grid_cell}/thumbnail.png\"\n",
    "        try:\n",
    "            obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "            img_bytes = obj['Body'].read()\n",
    "            img = Image.open(BytesIO(img_bytes))\n",
    "            return img\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def show_examples_per_cluster(df, unique_parquets, n_examples=3):\n",
    "    core_folders = []\n",
    "    for pq in unique_parquets:\n",
    "        try:\n",
    "            core_folders.append(resolve_core_folder(pq))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    if not core_folders:\n",
    "        print(\"Not found folder Core-S2L1C.\")\n",
    "        return\n",
    "\n",
    "    clusters = df['cluster'].unique()\n",
    "\n",
    "    for cl in clusters:\n",
    "        subset = df[df['cluster'] == cl].sample(\n",
    "            n=min(n_examples, (df['cluster'] == cl).sum()),\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(n_examples * 3, 3))\n",
    "        plt.suptitle(f\"Cluster {cl}\", y=1.05, fontsize=14)\n",
    "\n",
    "        for idx, (_, row) in enumerate(subset.iterrows()):\n",
    "            img = load_thumbnail_from_s3(row['grid_cell'], core_folders)\n",
    "            if img is None:\n",
    "                print(f\"No image for grid_cell: {row['grid_cell']}\")\n",
    "                continue\n",
    "\n",
    "            img_cropped = img.crop(row['pixel_bbox'])\n",
    "\n",
    "            ax = plt.subplot(1, n_examples, idx + 1)\n",
    "            ax.imshow(img_cropped)\n",
    "            ax.set_title(\n",
    "                f\"lat={row['centre_lat']:.5f}\\nlon={row['centre_lon']:.5f}\",\n",
    "                fontsize=9\n",
    "            )\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "show_examples_per_cluster(df_embeddings, unique_parquets, n_examples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def com(df, unique_parquets, cols=6, alpha=0.15):\n",
    "    \n",
    "    core_folders = []\n",
    "    for pq in unique_parquets:\n",
    "        try:\n",
    "            core_folders.append(resolve_core_folder(pq))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    if not core_folders:\n",
    "        print(\"Not found folder Core-S2L1C.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    grid_cells = df['grid_cell'].unique()\n",
    "    print(f\"Found {len(grid_cells)} unique grid_cells.\")\n",
    "\n",
    "\n",
    "    clusters = sorted(df['cluster'].unique())\n",
    "    cmap = plt.cm.get_cmap(\"tab20\", len(clusters))\n",
    "    cluster_colors = {cl: cmap(i) for i, cl in enumerate(clusters)}\n",
    "\n",
    "   \n",
    "    thumbs = []\n",
    "    for gc in grid_cells:\n",
    "        img = load_thumbnail_from_s3(gc, core_folders)\n",
    "        if img is None:\n",
    "            print(f\"No thumbnail for {gc}\")\n",
    "            continue\n",
    "        thumbs.append((gc, img))\n",
    "\n",
    "    \n",
    "    n = len(thumbs)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    plt.figure(figsize=(cols * 4, rows * 4))\n",
    "\n",
    "    for i, (gc, img) in enumerate(thumbs):\n",
    "        ax = plt.subplot(rows, cols, i + 1)\n",
    "        ax.imshow(img)\n",
    "\n",
    "        rows_for_gc = df[df['grid_cell'] == gc]\n",
    "\n",
    "        for _, row in rows_for_gc.iterrows():\n",
    "            cluster = row['cluster']\n",
    "            bbox = row['pixel_bbox']  \n",
    "\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1),\n",
    "                width,\n",
    "                height,\n",
    "                linewidth=1.5,\n",
    "                edgecolor=cluster_colors[cluster],\n",
    "                facecolor=cluster_colors[cluster],\n",
    "                alpha=alpha\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        ax.set_title(gc, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "com(df_embeddings, unique_parquets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395181e5",
   "metadata": {},
   "source": [
    "## Finding the most similar patches in clusters\n",
    "\n",
    "Steps\n",
    "\n",
    "1. Storing indices, cosine similarity, and location/grid cell info for pairs in the same cluster\n",
    "\n",
    "2. Selecting the pairs with highest similarity per cluster\n",
    "\n",
    "3. Listing the top pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910da0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pairs = defaultdict(list)\n",
    "N = len(df_embeddings)\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(i+1, N):\n",
    "        lat_i, lon_i = df_embeddings.iloc[i]['centre_lat'], df_embeddings.iloc[i]['centre_lon']\n",
    "        lat_j, lon_j = df_embeddings.iloc[j]['centre_lat'], df_embeddings.iloc[j]['centre_lon']\n",
    "\n",
    "        if lat_i == lat_j and lon_i == lon_j:\n",
    "            continue\n",
    "\n",
    "        cluster_i = df_embeddings.iloc[i]['cluster']\n",
    "        cluster_j = df_embeddings.iloc[j]['cluster']\n",
    "\n",
    "        if cluster_i != cluster_j:\n",
    "            continue\n",
    "\n",
    "        grid_cell_i = df_embeddings.iloc[i]['grid_cell']\n",
    "        grid_cell_j = df_embeddings.iloc[j]['grid_cell']\n",
    "\n",
    "        cluster_pairs[cluster_i].append((\n",
    "            i,\n",
    "            j,\n",
    "            cos_mat[i, j],\n",
    "            (lat_i, lon_i, grid_cell_i),\n",
    "            (lat_j, lon_j, grid_cell_j)\n",
    "        ))\n",
    "\n",
    "\n",
    "top_pairs_per_cluster = {}\n",
    "for cluster, pairs in cluster_pairs.items():\n",
    "    if pairs:\n",
    "        pairs_sorted = sorted(pairs, key=lambda x: x[2], reverse=True)[:5]\n",
    "        top_pairs_per_cluster[cluster] = pairs_sorted\n",
    "\n",
    "\n",
    "for cluster, pairs in top_pairs_per_cluster.items():\n",
    "    print(f\"CLUSTER {cluster}\")\n",
    "    for idx, (i, j, score, cell_i, cell_j) in enumerate(pairs, 1):\n",
    "        print(f\"  Pair {idx}: {i} and {j} | Cosine similarity: {score:.4f}\")\n",
    "        print(f\"    Patch 1: lat={cell_i[0]:.6f}, lon={cell_i[1]:.6f}, grid_cell={cell_i[2]}\")\n",
    "        print(f\"    Patch 2: lat={cell_j[0]:.6f}, lon={cell_j[1]:.6f}, grid_cell={cell_j[2]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77b4a1",
   "metadata": {},
   "source": [
    "## Images of the most similar patches in clusters\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Identifying the correct S3 folder for each parquet file and loading thumbnails for the grid cells\n",
    "\n",
    "2. Selecting the top pairs per cluster with the highest cosine similarity\n",
    "\n",
    "3. Cropping the images to the defined area and displaying them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee243e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='https://eodata.dataspace.copernicus.eu',\n",
    "#    aws_access_key_id='YOUR KEY',\n",
    "#    aws_secret_access_key='YOUR ACCESS',\n",
    "    region_name='default'\n",
    ")\n",
    "bucket_name = \"eodata\"\n",
    "\n",
    "def resolve_core_folder(parquet_path: str) -> str:\n",
    "    parquet_name = os.path.basename(parquet_path).replace(\".parquet\", \"\")\n",
    "    m = re.match(r\"part_(\\d+)-(\\d+)\", parquet_name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Error (name of parquet)\")\n",
    "    start = int(m.group(1))\n",
    "\n",
    "    core_prefix = \"auxdata/MajorTOM/Core-S2L1C/\"\n",
    "    resp = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=core_prefix, Delimiter=\"/\")\n",
    "    for cp in resp.get(\"CommonPrefixes\", []):\n",
    "        folder = cp[\"Prefix\"].rstrip(\"/\")\n",
    "        base = os.path.basename(folder)\n",
    "        m2 = re.match(r\"part_(\\d+)-(\\d+)\", base)\n",
    "        if not m2:\n",
    "            continue\n",
    "        left, right = int(m2.group(1)), int(m2.group(2))\n",
    "        if left <= start <= right:\n",
    "            return folder + \"/\"\n",
    "    raise ValueError(f\"Not found file for {parquet_path}\")\n",
    "\n",
    "def load_thumbnail_from_s3(grid_cell, core_folders):\n",
    "  \n",
    "    for folder in core_folders:\n",
    "        key = f\"{folder}{grid_cell}/thumbnail.png\"\n",
    "        try:\n",
    "            obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "            img_bytes = obj['Body'].read()\n",
    "            img = Image.open(BytesIO(img_bytes))\n",
    "            return img\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def show_top_pairs_per_cluster(df, top_pairs_per_cluster, unique_parquets, top_n=4):\n",
    "\n",
    "    core_folders = []\n",
    "    for pq in unique_parquets:\n",
    "        try:\n",
    "            core_folders.append(resolve_core_folder(pq))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    if not core_folders:\n",
    "        print(\"Not found file\")\n",
    "        return\n",
    "\n",
    "    for cluster, pairs in top_pairs_per_cluster.items():\n",
    "        for idx, (i, j, score, cell_i, cell_j) in enumerate(pairs[:top_n], 1):\n",
    "            row_i = df[(df['centre_lat']==cell_i[0]) & (df['centre_lon']==cell_i[1])].iloc[0]\n",
    "            row_j = df[(df['centre_lat']==cell_j[0]) & (df['centre_lon']==cell_j[1])].iloc[0]\n",
    "\n",
    "            bbox_i = row_i['pixel_bbox']\n",
    "            bbox_j = row_j['pixel_bbox']\n",
    "\n",
    "            img_i = load_thumbnail_from_s3(row_i['grid_cell'], core_folders)\n",
    "            img_j = load_thumbnail_from_s3(row_j['grid_cell'], core_folders)\n",
    "            if img_i is None or img_j is None:\n",
    "                print(f\"No images for {cluster}, pair {idx}\")\n",
    "                continue\n",
    "\n",
    "            img_i_cropped = img_i.crop(bbox_i)\n",
    "            img_j_cropped = img_j.crop(bbox_j)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
    "            fig.suptitle(f\"CLUSTER {cluster} - pair {idx}\", fontsize=12)\n",
    "\n",
    "            axes[0].imshow(img_i_cropped)\n",
    "            axes[0].set_title(f\"Patch 1:\\nlat={cell_i[0]:.6f}\\nlon={cell_i[1]:.6f}\", fontsize=10)\n",
    "            axes[0].axis('off')\n",
    "\n",
    "            axes[1].imshow(img_j_cropped)\n",
    "            axes[1].set_title(f\"Patch 2:\\nlat={cell_j[0]:.6f}\\nlon={cell_j[1]:.6f}\", fontsize=10)\n",
    "            axes[1].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            fig.subplots_adjust(top=0.85, bottom=0.15)\n",
    "            fig.text(0.5, 0.05, f\"Cosine similarity: {score:.4f}\", ha='center', fontsize=10)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "show_top_pairs_per_cluster(df_embeddings, top_pairs_per_cluster, unique_parquets, top_n=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
